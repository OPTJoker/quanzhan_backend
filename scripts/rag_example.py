"""
RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) å®ç°ç¤ºä¾‹
è¿™æ˜¯ç¬¬äºŒä¸ªå­¦ä¹ æ¨¡å— - æ„å»ºçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ
"""

import os
import json
from typing import List, Dict, Any
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
from langchain.document_loaders import TextLoader, PyPDFLoader
import mysql.connector
import redis
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class RAGSystem:
    """RAGçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        self.llm = OpenAI(
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            temperature=0.7
        )
        self.vectorstore = None
        self.qa_chain = None
        
        # æ•°æ®åº“è¿æ¥
        self.mysql_conn = mysql.connector.connect(
            host="localhost",
            user="ai_user",
            password="ai_pass",
            database="ai_development"
        )
        
        self.redis_conn = redis.Redis(
            host="localhost",
            port=6379,
            db=0,
            decode_responses=True
        )
    
    def load_documents(self, file_paths: List[str]) -> List[Any]:
        """åŠ è½½æ–‡æ¡£"""
        documents = []
        
        for file_path in file_paths:
            if file_path.endswith('.pdf'):
                loader = PyPDFLoader(file_path)
            else:
                loader = TextLoader(file_path, encoding='utf-8')
            
            docs = loader.load()
            documents.extend(docs)
        
        return documents
    
    def split_documents(self, documents: List[Any]) -> List[Any]:
        """åˆ†å‰²æ–‡æ¡£"""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        
        return text_splitter.split_documents(documents)
    
    def create_vectorstore(self, documents: List[Any]):
        """åˆ›å»ºå‘é‡æ•°æ®åº“"""
        chunks = self.split_documents(documents)
        
        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory="./data/chroma_db"
        )
        
        # æŒä¹…åŒ–å‘é‡æ•°æ®åº“
        self.vectorstore.persist()
        
        # åˆ›å»ºQAé“¾
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": 3}),
            return_source_documents=True
        )
    
    def load_vectorstore(self):
        """åŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“"""
        self.vectorstore = Chroma(
            persist_directory="./data/chroma_db",
            embedding_function=self.embeddings
        )
        
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": 3}),
            return_source_documents=True
        )
    
    def query(self, question: str) -> Dict[str, Any]:
        """æŸ¥è¯¢çŸ¥è¯†åº“"""
        if not self.qa_chain:
            return {"error": "RAGç³»ç»Ÿæœªåˆå§‹åŒ–"}
        
        # æ£€æŸ¥Redisç¼“å­˜
        cache_key = f"rag_query:{hash(question)}"
        cached_result = self.redis_conn.get(cache_key)
        
        if cached_result:
            return json.loads(cached_result)
        
        # æ‰§è¡ŒæŸ¥è¯¢
        try:
            result = self.qa_chain({"query": question})
            
            response = {
                "answer": result["result"],
                "sources": [doc.page_content[:200] + "..." for doc in result["source_documents"]],
                "metadata": [doc.metadata for doc in result["source_documents"]]
            }
            
            # ç¼“å­˜ç»“æœ
            self.redis_conn.setex(
                cache_key, 
                3600,  # 1å°æ—¶è¿‡æœŸ
                json.dumps(response, ensure_ascii=False)
            )
            
            # ä¿å­˜æŸ¥è¯¢å†å²åˆ°MySQL
            self.save_query_history(question, response["answer"])
            
            return response
            
        except Exception as e:
            return {"error": str(e)}
    
    def save_query_history(self, question: str, answer: str):
        """ä¿å­˜æŸ¥è¯¢å†å²åˆ°æ•°æ®åº“"""
        cursor = self.mysql_conn.cursor()
        
        query = """
        INSERT INTO rag_queries (question, answer, created_at) 
        VALUES (%s, %s, NOW())
        """
        
        cursor.execute(query, (question, answer))
        self.mysql_conn.commit()
        cursor.close()
    
    def add_document(self, content: str, title: str, source: str):
        """æ·»åŠ æ–°æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        # ä¿å­˜åˆ°MySQL
        cursor = self.mysql_conn.cursor()
        
        query = """
        INSERT INTO documents (title, content, source, created_at) 
        VALUES (%s, %s, %s, NOW())
        """
        
        cursor.execute(query, (title, content, source))
        self.mysql_conn.commit()
        cursor.close()
        
        # æ›´æ–°å‘é‡æ•°æ®åº“
        if self.vectorstore:
            from langchain.schema import Document
            
            doc = Document(page_content=content, metadata={"title": title, "source": source})
            self.vectorstore.add_documents([doc])
            self.vectorstore.persist()

def main():
    """ç¤ºä¾‹ç”¨æ³•"""
    rag_system = RAGSystem()
    
    # ç¤ºä¾‹1ï¼šåŠ è½½æ–‡æ¡£å¹¶åˆ›å»ºçŸ¥è¯†åº“
    print("ğŸ”„ æ­£åœ¨åˆ›å»ºçŸ¥è¯†åº“...")
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    sample_docs = [
        "AIäººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé¢†åŸŸï¼Œä¸“æ³¨äºè®¡ç®—æœºä¸äººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ã€‚"
    ]
    
    # åˆ›å»ºä¸´æ—¶æ–‡æ¡£æ–‡ä»¶
    os.makedirs("./data/docs", exist_ok=True)
    
    for i, content in enumerate(sample_docs):
        with open(f"./data/docs/doc_{i}.txt", "w", encoding="utf-8") as f:
            f.write(content)
    
    # åŠ è½½æ–‡æ¡£
    doc_files = [f"./data/docs/doc_{i}.txt" for i in range(len(sample_docs))]
    documents = rag_system.load_documents(doc_files)
    
    # åˆ›å»ºå‘é‡æ•°æ®åº“
    rag_system.create_vectorstore(documents)
    
    print("âœ… çŸ¥è¯†åº“åˆ›å»ºå®Œæˆ!")
    
    # ç¤ºä¾‹2ï¼šæŸ¥è¯¢çŸ¥è¯†åº“
    questions = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "NLPæ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    for question in questions:
        print(f"\nâ“ é—®é¢˜: {question}")
        result = rag_system.query(question)
        
        if "error" in result:
            print(f"âŒ é”™è¯¯: {result['error']}")
        else:
            print(f"ğŸ¤– å›ç­”: {result['answer']}")
            print(f"ğŸ“š æ¥æº: {len(result['sources'])} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")

if __name__ == "__main__":
    main()
